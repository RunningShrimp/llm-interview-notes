# AI大模型应用开发岗位面试知识点清单（含详实示例回答，完整版）

> 📅 更新时间：2024年12月（含补充：MCP / A2A / 视频→文本多模态 / Function Calling 进阶）  
> 🎯 适用范围：AI大模型应用开发工程师、LLM应用架构师、AI应用开发工程师  
> 📊 覆盖度：约80%的面试高频问题  
> 🔖 深度标记：了解概念 / 理解原理 / 能动手实现 / 能优化设计

---

## 📋 使用说明
- 先扫目录定位薄弱点，再结合项目经验准备“可讲的故事”与指标。
- 回答建议“总—分—总”，多用自己项目里的数据、对比和取舍理由。

---

## 一、大模型核心概念

### 1.1 大模型基础理论
- **1.1.1 大模型训练流程（理解原理）**  
  - 阶段：预训练（大规模通用语料，自监督 Causal LM/MLM）→ 指令微调（SFT，指令-回答对）→ 对齐（RLHF/RAI/ DPO/KTO，偏好数据→奖励模型→强化学习或直接偏好优化）。  
  - 关键：数据清洗去重（重复/脏词/泄露信息），分布一致的验证集；tokenizer 选型；训练监控（loss、梯度范数、学习率计划、early stop）；混合精度与梯度累积；checkpoint 策略与灾备。  
  - 产出：基座模型 + 对齐模型；对齐数据与奖励模型版本要可溯源。  
- **1.1.2 大模型核心能力（理解原理）**  
  - 涌现：参数/数据/算力到阈值后出现新能力（推理、多步指令）；核心：上下文理解、指令跟随、思维链推理、零/少样本泛化、多语言/多模态。  
  - 评估：通用基准（MMLU/BBH/GSM8K/HellaSwag）、安全基准（TruthfulQA/AdvBench）、指令跟随（AlpacaEval/IF eval），再加业务 golden set。  
- **1.1.3 参数与结构（理解原理）**  
  - 参数=权重/偏置；规模↑能力↑但推理/存储/带宽成本↑。  
  - 指令微调让模型“听话”；PEFT(LoRA/QLoRA)在资源可控下适配新任务。  
- **1.1.4 幻觉问题（能动手实现）**  
  - 成因：训练数据缺失或噪声、模型过度自信、检索缺失或召回偏差。  
  - 缓解：RAG 引入外部证据；提示中要求引用来源/“不知道就说不知道”；低温度/限制解码长度；结构化输出+规则/正则/Schema 校验；事实校验（LLM judge+规则）；高风险场景人工审核。

### 1.2 Transformer 架构
- **1.2.1 基础（理解原理）**  
  Encoder-Decoder（适合翻译）、Decoder-only（GPT，生成更高效）。组件：自注意力、FFN、残差、LayerNorm、位置编码/旋转位置编码(RoPE)。长上下文靠 RoPE/ALiBi/分块注意力/滑动窗口。  
- **1.2.2 注意力机制（理解原理）**  
  Self-Attention 公式中除以 √dk，避免点积随维度增大导致 softmax 过陡、梯度不稳；多头注意力=并行子空间关注；FlashAttention/v2 通过块化提升显存效率。  
- **1.2.3 变体（了解概念）**  
  Prefix LM(Enc-Dec) vs Causal LM(Dec-only)；主流开源：LLaMA3、Mistral/Mixtral、Qwen、GLM；优点：长依赖、并行；缺点：O(n²) 注意力开销、位置偏置。

### 1.3 Token 与 Embedding
- Token：BPE/WordPiece/SentencePiece；token 数直接影响成本与上下文上限。中文常更碎，需注意分词策略。  
- Embedding：选择看语言覆盖、维度/延迟/精度；常用 text-embedding-3-large/ada、bge-m3/bge-large-zh、m3e、sentence-BERT；多模态时用 CLIP/BLIP/LLaVA/Qwen-VL embedding；RAG 中 embedding 一致性与领域适配很关键。

---

## 二、技术栈与工具

### 2.1 主流大模型
- 商业 API（GPT-4/4o/Claude/Gemini）：看性能、上下文长度、价格、安全/合规；Function Calling/JSON 模式；多轮对话 token 成本需控。  
- 开源（LLaMA/Mistral/Qwen/ChatGLM/Baichuan）：看许可证、中文/多语能力、推理速度、量化友好度；部署用 vLLM/TensorRT-LLM；量化 AWQ/GPTQ/LLM.int8；私有化与数据不出域场景优先。

### 2.2 开发框架
- LangChain：链/Agent 编排，LCEL，TextSplitter，Output Parser，Memory；RAG 组件丰富但抽象层多，生产需控层级与观测。  
- LlamaIndex：数据/索引侧强，Index/QueryEngine/Retriever 抽象；与 LangChain 可互补。  
- Agent 框架：AutoGen（多 Agent 对话）、CrewAI（角色分工）、MCP（标准化工具/资源协议，见补充 2.5）。

### 2.3 向量数据库
- 相似度：cosine/IP/L2；索引：IVF/HNSW/PQ/IVF_PQ；元数据过滤与混合检索。  
- 选型：Milvus(分布式)、Pinecone(托管/Serverless)、Qdrant(Rust，高性能)、Weaviate(GraphQL+hybrid)、FAISS(本地库)、Chroma(轻量)。  
- 优化：HNSW 高 QPS；IVF_SQ8/PQ 降存储；分片+副本；冷热分层；并发写入的 WAL/compaction 策略；压测看延迟分布 P95/P99。

### 2.4 其他工具
- 推理优化：vLLM（PagedAttention+连续批处理）、TensorRT-LLM；KV Cache；量化(8/4bit)；批处理/多路复用；模型路由。  
- 微调：HF PEFT（LoRA/QLoRA/Prefix/AdaLoRA），关注 rank、α、dropout、bf16、FlashAttention，梯度检查与梯度裁剪，正则与早停。

### 2.5 MCP 与 A2A（补充）
- **MCP（Model Context Protocol）**  
  - 作用：统一模型—工具交互协议，显式 schema/权限，跨模型/框架可复用，易审计和路由。  
  - 设计点：工具/资源声明（JSON Schema）、上下文与权限、调用/返回格式、错误码；结合 tracing 记录调用链；配额与幂等。  
- **A2A（Assistant-to-Assistant，多助手协同）**  
  - 价值：角色分工（规划/执行/裁决）、并行与冗余验证（互检/投票）、提升稳健性。  
  - 工程：消息通道（队列/事件流）、上下文隔离、幂等与超时、冲突解决（投票/优先级/规则）、成本/时延上限、兜底单轨。

---

## 三、应用开发关键技术

### 3.1 Prompt 工程
- 基础：角色+指令+上下文+输入+输出格式；零样本 vs 少样本；分隔符与反例。  
- 高级：CoT、Self-Consistency、ToT、ReAct、Reflexion；结构化输出用 JSON/Schema/函数调用/语法约束解码。  
- 优化：A/B 测试；提示模板与版本管理；对抗提示注入（指令/数据隔离，系统提示封闭，输入净化）。

### 3.2 RAG（检索增强生成）
- 基本流程：文档切分（递归/语义）→ Embedding → 入库 → 检索 → 重排（cross-encoder/双塔再排）→ 组装 Prompt → 生成。  
- 优化：  
  - 检索：查询改写/扩展，hybrid（稠密+BM25/ColBERT），多路召回并行；  
  - 重排：交叉编码提高精度，代价大可对 top-k 精排；  
  - 多轮：对话摘要+标签化检索；  
  - 幻觉：强制引用、低温度、结构化输出后校验、答案内嵌来源。  
- 评估：P@k/Recall@k/MRR/NDCG；端到端事实性/相关性/流畅度，LLM-as-judge+人工；线上 A/B 看覆盖率、幻觉率、延迟、成本。

### 3.3 微调
- 选择：领域强/风格固定/低延迟需求 → 微调；知识频繁变 → 先 RAG。  
- PEFT：LoRA（低秩适配，显存友好）；QLoRA（4bit 基座 + LoRA，更省显存，适合单卡/小集群）。  
- 实践：  
  - 数据：清洗去重，指令-回答格式一致，避免模板化泄漏；  
  - 配置：rank/α/dropout/lr/epoch/batch/梯度累积；  
  - 防灾难遗忘：混合部分通用数据或正则；  
  - 评估：领域集 + 人工；监控过拟合（train/valid gap）。

### 3.4 Agent 开发
- 模式：ReAct（推理+行动）、计划-执行-反思；工具用 Function Calling/Schema。  
- 多 Agent：规划/执行/裁决分工；消息路由；幂等、超时与重试；冲突解决（投票/优先级/规则）；降级兜底单轨。  
- 评估：任务成功率、调用次数、时延、错误恢复、成本；常见坑：工具幻觉、死循环、无用调用。

### 3.5 视频→文本多模态（补充）
- 任务：长视频摘要/问答/章节抽取/事件检测。  
- 流程：  
  1) ASR 提取音轨（Whisper/Paraformer），保留时间戳；  
  2) 抽帧/分段（场景切换检测，均匀+关键帧），视觉编码（CLIP/ViT/Video-LLaVA/Qwen-VL-Video）；  
  3) 对齐：音频时间戳与帧时间戳对齐，生成分段 caption/embedding；  
  4) 检索+重排：对问题检索相关分段，交叉编码精排；  
  5) 生成：分段级回答/摘要 → 层次汇总为全局摘要/答案。  
- 成本控制：低分辨率/低帧率先粗览，重点片段再精析；分层摘要减少上下文；可对分段 embedding 做聚类/去冗余。  
- 评测：事实性（与字幕/脚本对齐）、覆盖率（召回关键片段）、延迟/成本、用户满意度。

---

## 四、工程实践与架构设计

### 4.1 应用架构
- 分层：前端 → API 网关 → 编排/业务服务 → 模型服务（路由/推理/缓存） → 数据层（向量库/DB/对象存储）。  
- RAG 架构：摄取流水线（清洗/切分/入库）+ 查询侧（检索/重排/生成）。  
- 可扩展：无状态服务+自动扩缩容；向量库分片/副本；多区域；异步队列。  
- 性能：多级缓存（提示裁剪、语义缓存、结果缓存）；批处理/连续批处理；KV Cache；小模型优先路由，大模型兜底。

### 4.2 部署与运维
- 部署：API 调用 vs 私有化部署（vLLM/Triton）；容器化(K8s)；蓝绿/金丝雀。  
- 成本：量化、批处理、缓存、分层路由；用量监控、限流、预算告警；热点预计算。  
- 监控：延迟/吞吐/错误率/成本，GPU/CPU/内存/网络，向量库/DB QPS/延迟；Tracing (OpenTelemetry)；SLO+告警。  
- 可靠性：断路器、重试/幂等、超时、优雅降级（小模型/静态答案/缓存命中），多活/备份，灾备演练。

### 4.3 安全与合规
- 数据：传输/静态加密，脱敏（PII）、RBAC，审计日志，数据留存与删除策略。  
- 模型安全：输入净化（黑白名单/正则）、防提示注入、输出过滤与安全检查，越狱测试集。  
- 合规：GDPR/CCPA/HIPAA；数据最小化、目的限制；用户访问/删除权；跨境数据传输合规。

### 4.4 测试与质量
- 测试：单测/集成/E2E；非确定性输出用“规则+统计”验证（正则/schema/模版相似度）；性能压测。  
- 评估：自动指标 + LLM-as-a-judge + 人工；检索指标；事实性与安全性基准。  
- 持续改进：日志→标注→增量微调/提示迭代；A/B 测试；CI/CD + MLOps（模型/数据版本化，实验跟踪 W&B/MLflow）。

---

## 五、场景设计与应用（示例回答框架）

- **企业知识库 RAG**：  
  架构=摄取(清洗/切分/embedding/入库) + 查询(检索/重排/生成)。难点=幻觉/延迟/成本。策略=hybrid 检索、交叉编码重排、引用来源、低温度、缓存与批处理、并行召回；监控覆盖率/幻觉率/延迟/成本。  
- **复杂工具调用 Agent**：  
  计划-执行-校验，工具 schema 显式；幂等键、超时与重试；并行化，失败兜底；记录调用链审计。  
- **代码助手**：  
  代码模型(CodeLlama/StarCoder)；多文件上下文；结构化输出（JSON/函数调用）；危险代码过滤；单测生成与解释。  
- **视频/多模态问答**：  
  ASR+抽帧分段→caption/embedding→检索+重排→层次摘要；对齐时间戳；成本分层；重点片段大模型精析。  
- **垂直合规（医疗/金融/法律）**：  
  数据脱敏、合规审计、人工审核闭环；专业基准/知识库；高风险输出拦截。

---

## 六、面试高频问题 FAQ（详实示例答案）

**Q1: 设计企业级 RAG 系统？**  
答：  
1) 架构分层：摄取（清洗/切分/embedding/入库向量库+对象存储）→ 查询（检索/重排/Prompt 组装/生成）→ 观测（日志/Tracing/指标）。  
2) 选型：领域 embedding（如 bge-m3/m3e/bge-large-zh），向量库（Qdrant/Milvus/Pinecone），LLM 路由（小模型优先，大模型兜底）。  
3) 提示：要求引用来源，结构化输出，必要时 JSON/Schema 校验。  
4) 优化：hybrid 检索，多路召回并行，交叉编码重排；缓存（语义/结果），批处理或连续批处理；低温度。  
5) 监控：覆盖率、幻觉率、延迟、成本；SLO+告警；蓝绿/金丝雀发布。  
6) 安全/合规：权限、审计日志，输入净化，敏感字段脱敏。  

**Q2: 如何评估 RAG 效果？**  
答：  
- 检索：P@k、Recall@k、MRR、NDCG；  
- 生成：事实性/相关性/流畅度，LLM-as-judge + 人工抽检；  
- 端到端：A/B 测试，用户满意度/转化；  
- 线上：覆盖率、幻觉率、延迟、成本，按人群/国家/版本分桶；  
- 数据：维护领域 golden set，迭代更新。  

**Q3: 选模型的决策流程？**  
答：  
- 任务与语言：中/英/多语，多模态与否；  
- 约束：数据隐私/上云政策、延迟目标、成本预算；  
- 评测：小样本基准（准确/推理/长上下文），延迟与价格；  
- 功能：是否需 Function Calling/JSON/长上下文；  
- 路由：多模型路由，按难度/长度/安全等级分流，小模型命中，大模型兜底；  
- 若需私有化：开源量化 + vLLM 部署，评估 GPU/内存/带宽。  

**Q4: 成本优化怎么做？**  
答：  
- 路由：小模型优先，大模型兜底；按任务/长度分层；  
- 模型：量化(8/4bit)，vLLM 连续批处理，多路复用；  
- Prompt：裁剪历史，摘要对话，压缩无关字段；  
- 缓存：语义缓存、结果缓存、热门查询预计算；  
- 运营：限流/配额，预算告警，离线批处理替代在线调用；  
- 数据：减少无效重试，合理超时；AB 验证成本/效果。  

**Q5: 如何降低幻觉？**  
答：  
- RAG 引证，强制引用来源；  
- 低温度/限制解码长度；  
- 重排提升相关性；  
- 结构化输出 + 校验（正则/Schema/规则）；  
- 事实性判别器或二次校验；  
- 高风险问题要求模型回答“未知”或转人工；  
- 数据侧：高质量语料/指令数据，减少噪声。  

**Q6: 微调 vs RAG？何时微调？**  
答：  
- RAG：知识更新快、可解释；适合 FAQ/文档问答/合规需可追溯。  
- 微调：领域强、风格统一、低延迟；适合结构化/公式化输出，或需要降低推理成本。  
- 选择：知识变动频繁→RAG；对话/风格/推理模式固定→微调(LoRA/QLoRA)。可组合：RAG 供实时知识，微调用于风格和推理模板。  

**Q7: 设计可扩展多 Agent 系统？**  
答：  
- 角色：Planner/Executors/Judge；  
- 流程：计划→并行执行→裁决/汇总；  
- 通道：队列/事件流，带会话/任务 ID；  
- 健壮性：幂等键、超时、重试、熔断；  
- 冲突解决：投票/优先级/规则；  
- 监控：成功率、调用次数、时延、成本，循环检测；  
- 兜底：单轨回退或静态答复。  

**Q8: 数十亿文档检索怎么设计？**  
答：  
- 向量库分片+副本，分区按语种/主题；  
- 多级索引：粗排（IVF/HNSW）+ 精排（交叉编码/重排）；  
- 存储：SSD + 足够内存，压缩（PQ/IVF_SQ8）；  
- 查询：查询改写/剪枝，限制 top-k；  
- 缓存：热点向量/查询；  
- 可用性：负载均衡、故障转移、数据重建；  
- 监控：QPS、延迟 P95/P99、召回/精度。  

**Q9: 服务突然变慢如何排查？**  
答：  
- 看监控：延迟/吞吐/错误率；  
- 资源：GPU/CPU/内存/网络；  
- 依赖：向量库/DB/API 延迟；  
- 日志+Tracing 定位卡点（模型推理、检索、重排、外部 API）；  
- 配置：批大小/并发/队列积压；  
- 变更：回滚近期版本/参数；  
- 临时降级：小模型路由/缓存兜底。  

**Q10: 提升 RAG 相关性？**  
答：  
- 查询改写/扩展，意图分类；  
- 领域 embedding/微调；  
- hybrid 检索（稠密+BM25/词法）；  
- 重排（交叉编码或 rerank 模型）；  
- 利用会话历史/用户画像；  
- 负样本挖掘训练检索/重排；  
- 反馈闭环：标注错误检索并迭代。  

**Q11: LoRA vs QLoRA？**  
答：  
- LoRA：低秩适配，基座全精度；显存中等。  
- QLoRA：先 4bit 量化基座，再加 LoRA；显存最省，适合单卡/小机；实现略复杂。  
- 选择：显存紧张/成本敏感→QLoRA；显存充足且想简化流程→LoRA。  

**Q12: Self-Attention 为什么除以 √dk？**  
答：避免点积随维度变大会让 softmax 过陡，分布饱和，梯度不稳；缩放后方差稳定，训练更平滑。

**Q13: 向量索引如何选？**  
答：  
- 中小规模高精度：IVF_FLAT 或 HNSW；  
- 超大规模省存储：IVF_SQ8/PQ；  
- 高 QPS 低延迟：HNSW；  
- 组合：IVF 粗排 + HNSW/重排；结合过滤需要支持倒排 + 向量。  

**Q14: 多轮对话上下文怎么管？**  
答：  
- 滑动窗口裁剪历史；  
- 对话摘要，关键信息入检索记忆；  
- 会话片段入向量库；  
- 遗忘/截断策略，防提示过长；  
- 结构化状态（槽位/标签）减少 token 开销。  

**Q15: Function Calling 原理？**  
答：提供工具的 JSON Schema，模型输出要调用的函数名与参数；应用侧校验后执行工具，返回结构化结果再让模型继续推理。关键：参数校验、幂等、超时重试、安全/权限/审计、防注入。

**Q16: Function Calling 进阶要点？**  
答：  
- Schema 即契约：类型/必填/枚举/正则/范围；  
- 最小工具集，避免“工具选择困难”；  
- 并行工具：模型列出调用清单，应用侧并行执行，统一汇总；  
- 错误返回机读 err_code/err_msg，便于二次推理/重试；  
- 安全：输入净化、权限/配额、敏感工具人工/规则闸；  
- 重试与降级：参数失败→模型重写参数；工具失败→退避重试/降级路径；  
- 可拆两步：先意图分类/工具选择，再参数填充。  

**Q17: 视频→文本问答/摘要怎么做？**  
答：  
1) ASR 提取音轨文字（带时间戳）；  
2) 抽帧/分段（场景变换检测+均匀采样），视觉编码（CLIP/Video-LLaVA/Qwen-VL-Video）；  
3) 对齐：按时间戳绑定文本与帧，生成分段 caption/embedding；  
4) 检索：对问题检索相关分段，交叉编码重排；  
5) 生成：分段回答/摘要 → 层次汇总；  
6) 成本：低分辨率粗览，重点片段精析；缓存/批处理；  
7) 评测：事实性（与字幕/脚本比对）、覆盖率、延迟/成本、用户满意度。

**Q18: MCP 是什么，有何价值？**  
答：模型—工具交互协议，标准化声明工具/资源/schema/权限；好处：跨模型/框架复用，安全可控（显式权限），便于审计与路由；适合多工具、多模型的大型系统。

**Q19: A2A 协同怎么做？**  
答：  
- 角色：Planner 产计划，Executors 执行，Judge 裁决/汇总；  
- 并行与冗余：同任务多执行者互检，或双轨（主执行 + 监督重写）；  
- 通信：队列/事件流，带上下文 ID；  
- 健壮性：幂等、超时、重试、熔断；  
- 冲突：投票/规则优先级；  
- 成本与时延上限，超限走兜底（单轨/静态答案）。

---

## 七、面试准备建议
- 技术：Python/深度学习框架；能写最小 RAG（切分→embedding→检索→重排→生成）；能做 LoRA/QLoRA；会用 vLLM 部署与压测。  
- 项目讲述：准备 2–3 个案例，包含目标、数据、架构、难点、指标、成本/安全/合规处理、迭代效果。  
- 练习：LeetCode 中等；准备 Prompt 改写提升案例；准备“日志→标注→迭代”闭环故事。  

---

## 八、学习资源
- 文档：Hugging Face、LangChain、LlamaIndex、OpenAI、Pinecone。  
- 论文：Attention Is All You Need（Transformer），GPT-3，InstructGPT，LoRA，RAG 论文。  
- 开源：LangChain，LlamaIndex，AutoGen，vLLM，Milvus/Qdrant。  

---

## 九、快速复盘清单
- 能否画出 RAG 全链路并指出性能/幻觉优化点？  
- 能否讲清 LoRA/QLoRA 差异与资源收益？  
- 能否设计监控+告警+降级策略？  
- 能否用实例说明如何降成本（路由/量化/缓存/批处理）？  
- 能否 5 分钟内给出最小 RAG/Agent/Function Calling 的实现思路？
